{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "879b3b75-a974-4a87-bd12-d1216dfce30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-03 22:33:51.326758: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-03 22:33:51.371686: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX512F AVX512_VNNI, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.25.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from transformers import (\n",
    "    GPT2LMHeadModel, \n",
    "    GPT2Tokenizer, \n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainerCallback\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21007720-aa66-47ec-85a7-0e6b90cc665b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set the seed\n",
    "set_seed(42)  # You can change this to any integer value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a720f265-63b8-40a9-8e2f-bb54d3081d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, block_size):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        tokenized_text = tokenizer.encode(text)\n",
    "        self.examples = [tokenized_text[i:i + block_size] for i in range(0, len(tokenized_text) - block_size + 1, block_size)]\n",
    "        print(f\"Loaded {len(self.examples)} examples.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return torch.tensor(self.examples[i], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b96b75f-1538-4e25-a390-ca8592bcf457",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "\n",
    "    def training_step(self, model, inputs):\n",
    "        loss = super().training_step(model, inputs)\n",
    "        self.train_loss.append(loss.item())\n",
    "        return loss\n",
    "    \n",
    "    def evaluation_loop(self, *args, **kwargs):\n",
    "        output = super().evaluation_loop(*args, **kwargs)\n",
    "        self.val_loss.append(output.metrics['eval_loss'])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0673f108-e4b7-4366-9ca3-389084183c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveEpochCallback(TrainerCallback):\n",
    "    def __init__(self, save_epochs, output_dir, tokenizer):\n",
    "        self.save_epochs = save_epochs\n",
    "        self.output_dir = output_dir\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        epoch = int(state.epoch)\n",
    "        print(f\"Callback triggered for epoch {epoch}\")\n",
    "        if epoch % self.save_epochs == 0:\n",
    "            checkpoint_dir = os.path.join(self.output_dir, f\"checkpoint-epoch-{epoch}\")\n",
    "            print(f\"Attempting to save checkpoint for epoch {epoch} to {checkpoint_dir}\")\n",
    "            if 'model' in kwargs:\n",
    "                kwargs['model'].save_pretrained(checkpoint_dir)\n",
    "                self.tokenizer.save_pretrained(checkpoint_dir)\n",
    "                print(f\"Saved checkpoint for epoch {epoch} to {checkpoint_dir}\")\n",
    "            else:\n",
    "                print(\"Model not found in kwargs, unable to save checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7c239d1-f21f-403e-ae58-3050e96a61bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, max_length=100):\n",
    "    input_ids = torch.tensor([[tokenizer.bos_token_id]]).to(model.device)\n",
    "    attention_mask = torch.ones_like(input_ids).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            no_repeat_ngram_size=2,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=0.75,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            bos_token_id=tokenizer.bos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True).strip()\n",
    "\n",
    "def generate_text_with_prompt(model, tokenizer, prompt, max_length=200):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(model.device)\n",
    "    attention_mask = torch.ones_like(input_ids).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=max_length + len(input_ids[0]),\n",
    "            num_return_sequences=1,\n",
    "            no_repeat_ngram_size=2,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=0.75,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            bos_token_id=tokenizer.bos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50ff08fd-1be5-405b-bb02-f2bc774d96d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/peft/tuners/lora/layer.py:1091: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"c_attn\", \"c_proj\"]\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a46842a-a34d-4ebd-9449-ed0021706ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"trump_speeches_combined_processed.txt\"\n",
    "block_size = 128\n",
    "full_dataset = TextDataset(file_path, tokenizer, block_size)\n",
    "\n",
    "train_size = int(0.9 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d22b124-0b62-4b0a-8562-3e0139ec767d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "total_epochs = 1000\n",
    "save_epochs = 10\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=total_epochs,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=0.001,\n",
    "    warmup_steps=100,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",  # We'll handle saving with our custom callback\n",
    "    fp16=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    load_best_model_at_end=False,  # We're not using the default saving strategy\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Pass the tokenizer to the callback\n",
    "save_callback = SaveEpochCallback(save_epochs, training_args.output_dir, tokenizer)\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[save_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddb706c-e3de-4932-8718-5b2c9756a0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476318d1-efdf-4855-bd73-75da1959c887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually save the final state if needed\n",
    "final_checkpoint_dir = os.path.join(training_args.output_dir, \"final-checkpoint\")\n",
    "trainer.save_model(final_checkpoint_dir)\n",
    "tokenizer.save_pretrained(final_checkpoint_dir)\n",
    "print(f\"Saved final checkpoint to {final_checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbdd89d-0f46-4900-b863-f8e39335390c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What are your thoughts on Kamala Harris running as president against you, do you think she'll win?\"\n",
    "\n",
    "for epoch in range(save_epochs, total_epochs + 1, save_epochs):\n",
    "    checkpoint_dir = f\"./results/checkpoint-epoch-{epoch}\"\n",
    "    if os.path.exists(checkpoint_dir):\n",
    "        loaded_model = GPT2LMHeadModel.from_pretrained(checkpoint_dir)\n",
    "        loaded_model = get_peft_model(loaded_model, peft_config)\n",
    "        loaded_model.to(device)\n",
    "        \n",
    "        print(f\"\\nGenerated text after epoch {epoch}:\")\n",
    "        print(generate_text(loaded_model, tokenizer))\n",
    "\n",
    "        print(f\"\\nGenerated text with prompt after epoch {epoch}:\")\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(f\"Response: {generate_text_with_prompt(loaded_model, tokenizer, prompt)}\")\n",
    "\n",
    "# Calculate final perplexity on validation set\n",
    "val_loss = trainer.evaluate()['eval_loss']\n",
    "val_perplexity = math.exp(val_loss)\n",
    "print(f\"Final Validation Perplexity: {val_perplexity:.2f}\")\n",
    "\n",
    "# Generate text with final model\n",
    "print(\"\\nFinal generated text:\")\n",
    "print(generate_text(model, tokenizer))\n",
    "\n",
    "print(\"\\nFinal generated text with prompt:\")\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Response: {generate_text_with_prompt(model, tokenizer, prompt)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d519b847-df0a-4bb2-a44d-eefb7d32d4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output_dir = \"./fine_tuned_gpt2_final\"\n",
    "trainer.save_model(final_output_dir)\n",
    "tokenizer.save_pretrained(final_output_dir)\n",
    "print(f\"Final model saved to {final_output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21c82b8-5e9f-4038-92c7-189c4aee0e2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
