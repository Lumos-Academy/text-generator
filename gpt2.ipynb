{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "879b3b75-a974-4a87-bd12-d1216dfce30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from transformers import (\n",
    "    GPT2LMHeadModel, \n",
    "    GPT2Tokenizer, \n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainerCallback\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "21007720-aa66-47ec-85a7-0e6b90cc665b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set the seed\n",
    "set_seed(42)  # You can change this to any integer value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a720f265-63b8-40a9-8e2f-bb54d3081d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, block_size):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        tokenized_text = tokenizer.encode(text)\n",
    "        self.examples = [tokenized_text[i:i + block_size] for i in range(0, len(tokenized_text) - block_size + 1, block_size)]\n",
    "        print(f\"Loaded {len(self.examples)} examples.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return torch.tensor(self.examples[i], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b96b75f-1538-4e25-a390-ca8592bcf457",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "\n",
    "    def training_step(self, model, inputs):\n",
    "        loss = super().training_step(model, inputs)\n",
    "        self.train_loss.append(loss.item())\n",
    "        return loss\n",
    "    \n",
    "    def evaluation_loop(self, *args, **kwargs):\n",
    "        output = super().evaluation_loop(*args, **kwargs)\n",
    "        self.val_loss.append(output.metrics['eval_loss'])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0673f108-e4b7-4366-9ca3-389084183c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveEpochCallback(TrainerCallback):\n",
    "    def __init__(self, save_epochs, output_dir, tokenizer):\n",
    "        self.save_epochs = save_epochs\n",
    "        self.output_dir = output_dir\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        epoch = int(state.epoch)\n",
    "        print(f\"Callback triggered for epoch {epoch}\")\n",
    "        if epoch % self.save_epochs == 0:\n",
    "            checkpoint_dir = os.path.join(self.output_dir, f\"checkpoint-epoch-{epoch}\")\n",
    "            print(f\"Attempting to save checkpoint for epoch {epoch} to {checkpoint_dir}\")\n",
    "            if 'model' in kwargs:\n",
    "                kwargs['model'].save_pretrained(checkpoint_dir)\n",
    "                self.tokenizer.save_pretrained(checkpoint_dir)\n",
    "                print(f\"Saved checkpoint for epoch {epoch} to {checkpoint_dir}\")\n",
    "            else:\n",
    "                print(\"Model not found in kwargs, unable to save checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b7c239d1-f21f-403e-ae58-3050e96a61bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, max_length=100):\n",
    "    input_ids = torch.tensor([[tokenizer.bos_token_id]]).to(model.device)\n",
    "    attention_mask = torch.ones_like(input_ids).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            no_repeat_ngram_size=2,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=0.75,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            bos_token_id=tokenizer.bos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True).strip()\n",
    "\n",
    "def generate_text_with_prompt(model, tokenizer, prompt, max_length=200):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(model.device)\n",
    "    attention_mask = torch.ones_like(input_ids).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=max_length + len(input_ids[0]),\n",
    "            num_return_sequences=1,\n",
    "            no_repeat_ngram_size=2,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=0.75,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            bos_token_id=tokenizer.bos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "50ff08fd-1be5-405b-bb02-f2bc774d96d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/peft/tuners/lora/layer.py:1091: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"c_attn\", \"c_proj\"]\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5a46842a-a34d-4ebd-9449-ed0021706ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1022810 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7990 examples.\n"
     ]
    }
   ],
   "source": [
    "file_path = \"trump_speeches_combined_processed.txt\"\n",
    "block_size = 128\n",
    "full_dataset = TextDataset(file_path, tokenizer, block_size)\n",
    "\n",
    "train_size = int(0.9 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9d22b124-0b62-4b0a-8562-3e0139ec767d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Training setup\n",
    "total_epochs = 1000\n",
    "save_epochs = 10\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=total_epochs,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=0.001,\n",
    "    warmup_steps=100,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",  # We'll handle saving with our custom callback\n",
    "    fp16=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    load_best_model_at_end=False,  # We're not using the default saving strategy\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Pass the tokenizer to the callback\n",
    "save_callback = SaveEpochCallback(save_epochs, training_args.output_dir, tokenizer)\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[save_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddb706c-e3de-4932-8718-5b2c9756a0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "476318d1-efdf-4855-bd73-75da1959c887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved final checkpoint to ./results/final-checkpoint\n"
     ]
    }
   ],
   "source": [
    "# Manually save the final state if needed\n",
    "final_checkpoint_dir = os.path.join(training_args.output_dir, \"final-checkpoint\")\n",
    "trainer.save_model(final_checkpoint_dir)\n",
    "tokenizer.save_pretrained(final_checkpoint_dir)\n",
    "print(f\"Saved final checkpoint to {final_checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7dbdd89d-0f46-4900-b863-f8e39335390c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated text after epoch 100:\n",
      ", the latest on the rise of anti-immigration sentiment. (Peter Stevenson/The Washington Post)\n",
      "\n",
      "In a statement released Monday, Trump's campaign announced that it had hired an outside lawyer to assist the campaign and that its senior staff was \"working on legal issues,\" with the final deadline to hire an attorney by July 14.\n",
      "- 'It's not the government that's paying attention to the president's tweets,' says former GOP strategist\n",
      ", former Trump campaign adviser, and Trump loyal\n",
      "\n",
      "Generated text with prompt after epoch 100:\n",
      "Prompt: What are your thoughts on free healthcare for all Americans?\n",
      "Response: What are your thoughts on free healthcare for all Americans? Let us know in the comments below!\n",
      "\n",
      "Images via Flickr, Shutterstock, and Creative Commons.\n",
      "\n",
      "Generated text after epoch 200:\n",
      "Bruno Massaro is an American singer and songwriter, and he is one of the most famous and influential artists of his generation. His solo work has made him one the biggest and most influential solo stars in pop music and the world.\n",
      "\n",
      "He is currently playing with his friend and fellow country singer, the band \"Sugar Shack\" at his house. He has sold over 20 million albums and is now one among the top 50 most popular acts in the United States.\n",
      "\n",
      "Generated text with prompt after epoch 200:\n",
      "Prompt: What are your thoughts on free healthcare for all Americans?\n",
      "Response: What are your thoughts on free healthcare for all Americans?\n",
      "\n",
      "Share your comments below.\n",
      ".\n",
      "\n",
      "Generated text after epoch 300:\n",
      "It was a great day for the Toronto Raptors, who had a 10-2 win over the Chicago Bulls in Game 3 of the Eastern Conference Semifinals.\n",
      "\n",
      "The Raptors scored the first two points of regulation in the opening frame before trailing by as many as 11 at the break. After a flurry of free throws, the Bulls were outscored by 9-3. But despite the play, Toronto made it 5-1. The Raptors went on to win 15-14 on\n",
      "\n",
      "Generated text with prompt after epoch 300:\n",
      "Prompt: What are your thoughts on free healthcare for all Americans?\n",
      "Response: What are your thoughts on free healthcare for all Americans?\n",
      "\n",
      "I agree with many of you. Free healthcare is a good thing. I am not against it. But for the same reason that you would like to have free health care, it is not free.\n",
      ":)\n",
      "\n",
      "Generated text after epoch 400:\n",
      "TRAVEL OF THE MESSENGER\n",
      "\n",
      "If you have ever read the book of the MSS. I am the author. You are welcome to skip to the end and read. The entire book is here.\n",
      "\n",
      "\n",
      "The only thing that is missing is a \"Journey to Mars\" book. It's a much bigger book, and I had a lot of fun doing it. If you want to know more about the mission, read on.\n",
      "\n",
      "Generated text with prompt after epoch 400:\n",
      "Prompt: What are your thoughts on free healthcare for all Americans?\n",
      "Response: What are your thoughts on free healthcare for all Americans? Have you been affected by the Affordable Care Act? What can you do to make sure you stay safe?\n",
      "\n",
      "Generated text after epoch 500:\n",
      "By\n",
      "\n",
      "As the world's oldest and most celebrated Catholic church, the Church of England, has been a powerful voice for LGBT rights. In the past, a group of prominent Catholic clergy, who were not members of the church at the time, have called for the repeal of sodomy laws.\n",
      ", and have been outspoken about the right of Catholics to be married to other non-Catholics. The Church has also been vocal about how it would like to give same-sex couples\n",
      "\n",
      "Generated text with prompt after epoch 500:\n",
      "Prompt: What are your thoughts on free healthcare for all Americans?\n",
      "Response: What are your thoughts on free healthcare for all Americans?\n",
      "\n",
      "The Affordable Care Act (ACA) is a win for the American people. It's a huge win and we want to win more of it. The ACA will be good for everybody. We've done a lot of work in the past to get it right.\n",
      ": I do think it's good that people are getting sicker and healthier. I think that's bad. But the ACA's not going to be a miracle. And what we can do is figure out what happens. What we're doing is taking the right steps to keep people healthier, reduce the costs for insurance companies, and make sure they get their fair share of the health care that we need to ensure that everyone has health coverage. That is what this is all about, that is our mission.\n",
      "\n",
      "Generated text after epoch 600:\n",
      "Fishermen in the Great Lakes region are scrambling to find the best water for their cattle, which is what they're looking for now.\n",
      "\n",
      "The Midwest-based company is hoping to open a new plant in Minnesota, but it's still a bit early in its development, according to Rick HernÃ¡ndez, president of the Lake Michigan Water Conservation District. That plant has yet to be built, though.\n",
      "\n",
      "\n",
      "\"I don't think it'll be done until we have an\n",
      "\n",
      "Generated text with prompt after epoch 600:\n",
      "Prompt: What are your thoughts on free healthcare for all Americans?\n",
      "Response: What are your thoughts on free healthcare for all Americans? What are the questions that need answering?\n",
      "\n",
      "Generated text after epoch 700:\n",
      "In a new report, the Center for Democracy and Technology at Columbia University asked American companies and policymakers to help make sure their lobbying efforts against Obamacare are more aligned with what's happening in the Senate and House.\n",
      "\n",
      "\"While the public is increasingly confused about the health care law, there is no doubt that more Americans are using their voices and knowledge to stand up and defend the government's role in providing health insurance to millions of Americans,\" says the report. \"More Americans than ever believe that the\n",
      "\n",
      "Generated text with prompt after epoch 700:\n",
      "Prompt: What are your thoughts on free healthcare for all Americans?\n",
      "Response: What are your thoughts on free healthcare for all Americans? Let us know in the comments section below!\n",
      "\n",
      "Generated text after epoch 800:\n",
      "The first time it was ever claimed that a terrorist attack was perpetrated on Americans by the Obama administration, the attack on the US consulate in Benghazi, Libya, was a pretext for a \"surprise attack.\" It was the first terror attack in US history.\n",
      "\n",
      "A little more than a week later, on October 16, 2013, a gunman killed Ambassador Chris Stevens, three others and wounded four more. The perpetrators of that attack were all American citizens. It's a tragedy for the American\n",
      "\n",
      "Generated text with prompt after epoch 800:\n",
      "Prompt: What are your thoughts on free healthcare for all Americans?\n",
      "Response: What are your thoughts on free healthcare for all Americans? Let us know your experiences in the comments section below.\n",
      "\n",
      "Photo Credit: Shutterstock.com\n",
      "\n",
      "Generated text after epoch 900:\n",
      "On the morning of September 14, 1984, an 8-year-old girl was playing in a playground in her neighborhood. The mother of the child, who was not named, heard a loud crack and a bang. As she went down, she heard the sound of a woman, a 6-foot-7 tall woman dressed in black, screaming.\n",
      "\n",
      "Her mother, Mary E. Robinson, saw the screaming child and took her to a local hospital. There, the baby was\n",
      "\n",
      "Generated text with prompt after epoch 900:\n",
      "Prompt: What are your thoughts on free healthcare for all Americans?\n",
      "Response: What are your thoughts on free healthcare for all Americans?\n",
      "\n",
      "Share your experiences.\n",
      "Final Validation Perplexity: 15.28\n",
      "\n",
      "Final generated text:\n",
      "â€™s not going to happen. i think weâ€œre going through a very rough year. we have to do it.\n",
      "weâ€–re not doing it anymore. so i just want to say that is a big day. itâ€…s a great day for all of us. everybody has to be treated fairly. and i want the press to see that iâ€£m the one that covers the issues. the media â€“ the fake news â€“ doesnâ€”t do\n",
      "\n",
      "Final generated text with prompt:\n",
      "Prompt: What are your thoughts on free healthcare for all Americans?\n",
      "Response: What are your thoughts on free healthcare for all Americans? i have a lot of respect for you, but i don't care what you think.\n",
      "we will always protect patients with pre-existing conditions and ensure equal access to care for everyone. there will be no discrimination. we will defend religious liberty and the right to keep and bear arms. and we'll strike down terrorists who threaten our citizens and they threaten civil rights. i'll never do that. our second amendment is under siege. you know that, right? and nobody talks about it, because nobody cares. nobody even talks. [applause] we're going to defend the second and it's under assault. it really is. so if we had to go, you'd be talking about constitutional amendments. if you go to new hampshire, i think you might be going, \"what are these people going through?\" and this is a new, new world. they're all sitting there, they have no idea what they've done. \"oh, what's going on.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What are your thoughts on free healthcare for all Americans?\"\n",
    "interval = 100\n",
    "for epoch in range(interval, total_epochs + 1, interval):\n",
    "    checkpoint_dir = f\"./results/checkpoint-epoch-{epoch}\"\n",
    "    if os.path.exists(checkpoint_dir):\n",
    "        loaded_model = GPT2LMHeadModel.from_pretrained(checkpoint_dir)\n",
    "        loaded_model = get_peft_model(loaded_model, peft_config)\n",
    "        loaded_model.to(device)\n",
    "        \n",
    "        print(f\"\\nGenerated text after epoch {epoch}:\")\n",
    "        print(generate_text(loaded_model, tokenizer))\n",
    "\n",
    "        print(f\"\\nGenerated text with prompt after epoch {epoch}:\")\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(f\"Response: {generate_text_with_prompt(loaded_model, tokenizer, prompt)}\")\n",
    "\n",
    "# Calculate final perplexity on validation set\n",
    "val_loss = trainer.evaluate()['eval_loss']\n",
    "val_perplexity = math.exp(val_loss)\n",
    "print(f\"Final Validation Perplexity: {val_perplexity:.2f}\")\n",
    "\n",
    "# Generate text with final model\n",
    "print(\"\\nFinal generated text:\")\n",
    "print(generate_text(model, tokenizer))\n",
    "\n",
    "print(\"\\nFinal generated text with prompt:\")\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Response: {generate_text_with_prompt(model, tokenizer, prompt)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d519b847-df0a-4bb2-a44d-eefb7d32d4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model saved to ./fine_tuned_gpt2_final\n"
     ]
    }
   ],
   "source": [
    "final_output_dir = \"./fine_tuned_gpt2_final\"\n",
    "trainer.save_model(final_output_dir)\n",
    "tokenizer.save_pretrained(final_output_dir)\n",
    "print(f\"Final model saved to {final_output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21c82b8-5e9f-4038-92c7-189c4aee0e2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
