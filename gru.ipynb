{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e2ae6ff-0423-4918-a52e-28cf56e34d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "816ad142-b91d-466f-9ada-7631ea43be31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set the seed\n",
    "set_seed(42)  # You can change this to any integer value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "004d87cc-417e-4872-be44-9eeebdc05f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9c1536b-4680-4e9f-9ec5-db03fe3a3176",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, file_path, seq_length, vocab=None):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        text = re.sub(r'\\s+', ' ', text)  # Clean up spaces\n",
    "        self.vocab = vocab or sorted(set(text))\n",
    "        self.char2idx = {c: i for i, c in enumerate(self.vocab)}\n",
    "        self.idx2char = {i: c for i, c in enumerate(self.vocab)}\n",
    "        self.seq_length = seq_length\n",
    "        self.text_as_int = np.array([self.char2idx[c] for c in text])\n",
    "        self.num_samples = len(self.text_as_int) - seq_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_seq = self.text_as_int[idx:idx + self.seq_length]\n",
    "        target_seq = self.text_as_int[idx + 1:idx + self.seq_length + 1]\n",
    "        return torch.tensor(input_seq, dtype=torch.long), torch.tensor(target_seq, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bce2956c-31c7-4914-ac8a-07b6a14d2815",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.gru = nn.GRU(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        out, hidden = self.gru(x, hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(num_layers, batch_size, hidden_size).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79825db5-3f7f-403f-9f5c-975dc4f72100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_generate(model, dataloader, criterion, optimizer, num_epochs, generation_length, generate_every_n_epochs):\n",
    "    model.train()\n",
    "    print(\"Started training\")\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            hidden = model.init_hidden(inputs.size(0))\n",
    "            \n",
    "            outputs, hidden = model(inputs, hidden)\n",
    "            hidden = hidden.detach()\n",
    "            loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f'Epoch {epoch}/{num_epochs}, Loss: {loss.item()}')\n",
    "        \n",
    "        if epoch % generate_every_n_epochs == 0:\n",
    "            model.eval()\n",
    "            generated_text = generate_text(model, generation_length)\n",
    "            print(f\"Generated text after {epoch} epochs:\\n{generated_text}\\n\")\n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1126b9d9-1849-4414-9707-5d7d7a7b65b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, generation_length, temperature=0.5):\n",
    "    model.eval()\n",
    "    generated_text = \"\"\n",
    "    input_seq = torch.tensor([[random.randint(0, vocab_size-1)]], dtype=torch.long).to(device)\n",
    "    hidden = model.init_hidden(1)\n",
    "    \n",
    "    for _ in range(generation_length):\n",
    "        with torch.no_grad():\n",
    "            output, hidden = model(input_seq, hidden)\n",
    "            output = output.div(temperature).exp()\n",
    "            next_char_idx = torch.multinomial(output[0, -1], 1).item()\n",
    "            next_char = idx2char[next_char_idx]\n",
    "            generated_text += next_char\n",
    "            input_seq = torch.tensor([[next_char_idx]], dtype=torch.long).to(device)\n",
    "    \n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c864cf28-164f-4666-a984-7d706a9a19d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started training\n",
      "Epoch 1/50, Loss: 0.8258545994758606\n",
      "Epoch 2/50, Loss: 0.9102525115013123\n",
      "Epoch 3/50, Loss: 0.8398287892341614\n",
      "Epoch 4/50, Loss: 0.8924387693405151\n",
      "Epoch 5/50, Loss: 0.8721145391464233\n",
      "Generated text after 5 epochs:\n",
      " or 2,000 years ago, the democrats are coming in from china. we have the support of the united states is now in her change and they all start fifty, and enforce the right people. this is a big thing. i don't want to just read it. they have to create a fair, safe, sane, i said, \"who is a disaster and they could rebuild its audience and they say it's so strongly into the clinton campaign. it’s going to be a lot of money for the farmers. he said her doin today. [applause] and we will make america g\n",
      "\n",
      "Epoch 6/50, Loss: 0.9931888580322266\n",
      "Epoch 7/50, Loss: 1.4136778116226196\n",
      "Epoch 8/50, Loss: 1.3076834678649902\n",
      "Epoch 9/50, Loss: 2.213380813598633\n",
      "Epoch 10/50, Loss: 1.355588436126709\n",
      "Generated text after 10 epochs:\n",
      "4 we have to people the way, we’re going to be another control beaution in the law en in the really rementer back for many of the companies again. we will never talk about the country and the more and we have to really time a startly and the world, it was a lot of for the consideral to resters of the million war the restrillion together and the way the world. we’re going to be any community in the way the deal our country and we were the money. thank you, your country that. the world product on \n",
      "\n",
      "Epoch 11/50, Loss: 1.2545223236083984\n",
      "Epoch 12/50, Loss: 1.2680401802062988\n",
      "Epoch 13/50, Loss: 1.2321786880493164\n",
      "Epoch 14/50, Loss: 1.2068310976028442\n",
      "Epoch 15/50, Loss: 1.2952942848205566\n",
      "Generated text after 15 epochs:\n",
      "etter back, they were a lot of the new region democrats and it can make america great care if i will be care dollars are our the new york that do young did i american people that was in the united states of the people a lot nation, and the really politicians; they only a little very much, it talks to me to the end of the friends of working that party of the factorict for a problem and if i go to a for the world and on the fairs of indianal and the great from the true care to me mexico for our in\n",
      "\n",
      "Epoch 16/50, Loss: 2.223372220993042\n",
      "Epoch 17/50, Loss: 2.3257033824920654\n",
      "Epoch 18/50, Loss: 2.3081042766571045\n",
      "Epoch 19/50, Loss: 2.375941514968872\n",
      "Epoch 20/50, Loss: 2.4321465492248535\n",
      "Generated text after 20 epochs:\n",
      " weching toon the g they so sat  alea this of the ialled and thic and touped to that ofrnt theyryt a be sarknt tht ye i we int the aou me pou waing to tht ite e ere g ere opr ttat bu and ant to bon t ece an it a and tht the she and de the ps gor th of cour bearon waut the even. andoe he bes we whe the su aoure that alli oea ve've ise exa ynd wo rorre ve ont  or it a andet i and oat y the the we the aouther a soo  wat doe mr i perat i bo anow we we the sut he gand the iand ae moret wif hall willa\n",
      "\n",
      "Epoch 21/50, Loss: 2.4091532230377197\n",
      "Epoch 22/50, Loss: 2.4484429359436035\n",
      "Epoch 23/50, Loss: 2.4578499794006348\n",
      "Epoch 24/50, Loss: 2.2155442237854004\n",
      "Epoch 25/50, Loss: 1.9795148372650146\n",
      "Generated text after 25 epochs:\n",
      " that you sand of the to whe abe have tho are that for hat and the the the do you the the premedaid the so have that und i will the we do wat hesty the way ther p and to kill the the the popole berelliosione bere the cand i tring that the they'reat have and i that sanc rean the it are we have meraton you hon the like be a have the going to he ch gre the we, the was my and of and the think a to even to bet the whount you know have and saring it becaust the the we on whe what the be got we of mor \n",
      "\n",
      "Epoch 26/50, Loss: 1.500097393989563\n",
      "Epoch 27/50, Loss: 1.3715873956680298\n",
      "Epoch 28/50, Loss: 1.3099870681762695\n",
      "Epoch 29/50, Loss: 1.3044735193252563\n",
      "Epoch 30/50, Loss: 1.2771060466766357\n",
      "Generated text after 30 epochs:\n",
      ",000 for the talking the president to the world. i was not say in the hear and some the world to the washingtons in a people. we have a great the new mike and i had a new wealth here in the deals, i think it was a lot of the dealies and china back to two because and started to she was going to put the first reason in the me done of the people in the our president to the restor with the real the row that congressme companies and doctor that said, \"they said, \"well, the world. we will be one the w\n",
      "\n",
      "Epoch 31/50, Loss: 1.2885687351226807\n",
      "Epoch 32/50, Loss: 1.2933329343795776\n",
      "Epoch 33/50, Loss: 1.285199761390686\n",
      "Epoch 34/50, Loss: 1.3114334344863892\n",
      "Epoch 35/50, Loss: 1.3332443237304688\n",
      "Generated text after 35 epochs:\n",
      ". they get everybody said the great american really happen deals with a lot of the world, one and i want to crime the religious deal. and it's not want to the world, we handing the frankly, china's a really because they say it about the greates politicians. and we have the people working in amazing soleading the best question of the support president we all me you a really say our great charge of them to the world. the united states, remember that because i'm a very really better wanted to have \n",
      "\n",
      "Epoch 36/50, Loss: 1.2359306812286377\n",
      "Epoch 37/50, Loss: 1.242428183555603\n",
      "Epoch 38/50, Loss: 1.2351746559143066\n",
      "Epoch 39/50, Loss: 1.2556580305099487\n",
      "Epoch 40/50, Loss: 1.262817144393921\n",
      "Generated text after 40 epochs:\n",
      "t they said that we can all the way, it's controlled and said, it's not going to be ever lost the beautiful and they're the the problems. we don't know what we will say they don't believe that it was all of our and while out of building and i would lead our country. they want to walk the best of the money. and the country. they want to the communitiens of the middle east administration is the politicians and they're going to be beat we've been wall of the worst state of hillary clinton back are \n",
      "\n",
      "Epoch 41/50, Loss: 1.2727420330047607\n",
      "Epoch 42/50, Loss: 1.2633376121520996\n",
      "Epoch 43/50, Loss: 1.2842323780059814\n",
      "Epoch 44/50, Loss: 1.2234339714050293\n",
      "Epoch 45/50, Loss: 1.2703477144241333\n",
      "Generated text after 45 epochs:\n",
      "- they said they have a repeal the headed about the cord of the best our country of american so i don't know the us and i'm the believe she was anybody was the president their country of the country. but i did with the school probably want to be down an about the whole deal and they're in the middly churging our person who have to work you people has so much for the words that i want to be the world. they were comings about my again. they don't know have the american people are going to tell you\n",
      "\n",
      "Epoch 46/50, Loss: 1.2268086671829224\n",
      "Epoch 47/50, Loss: 1.3046698570251465\n",
      "Epoch 48/50, Loss: 1.2166216373443604\n",
      "Epoch 49/50, Loss: 1.2885408401489258\n",
      "Epoch 50/50, Loss: 1.2881044149398804\n",
      "Generated text after 50 epochs:\n",
      " got to be donald trump war the life to really they were going to mexico, the world by the terrible that have been the united states and we leader the person. i say one of the gues that said that amazing one of this is interesting the people we want to have to be so very beautiful more and they got the problem to the moted in the thing and we think it was a lot of they don't republicans that was the wall complaines what we have to be one of the democrats with the most is the medicare. because i \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "file_path = 'trump_speeches_combined_processed.txt'\n",
    "seq_length = 100\n",
    "embed_size = 128\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "batch_size = 64\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "generation_length = 500\n",
    "generate_every_n_epochs = 5\n",
    "\n",
    "# Initialize Dataset and DataLoader\n",
    "dataset = TextDataset(file_path, seq_length)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# Vocabulary size\n",
    "vocab_size = len(dataset.vocab)\n",
    "char2idx = dataset.char2idx\n",
    "idx2char = dataset.idx2char\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = GRUModel(vocab_size, embed_size, hidden_size, num_layers).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model and generate text in a loop\n",
    "train_and_generate(model, dataloader, criterion, optimizer, num_epochs, generation_length, generate_every_n_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
